2024-12-19 17:46:09 [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.
2024-12-19 17:46:09 
2024-12-19 17:46:09 [GIN-debug] [WARNING] Running in "debug" mode. Switch to "release" mode in production.
2024-12-19 17:46:09  - using env:export GIN_MODE=release
2024-12-19 17:46:09  - using code:gin.SetMode(gin.ReleaseMode)
2024-12-19 17:46:09 
2024-12-19 17:46:09 2024/12/20 01:46:09 routes.go:1259: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
2024-12-19 17:46:09 time=2024-12-20T01:46:09.619Z level=INFO source=images.go:757 msg="total blobs: 34"
2024-12-19 17:46:09 time=2024-12-20T01:46:09.724Z level=INFO source=images.go:764 msg="total unused blobs removed: 0"
2024-12-19 17:46:09 time=2024-12-20T01:46:09.828Z level=INFO source=routes.go:1310 msg="Listening on [::]:11434 (version 0.5.4-0-g2ddc32d-dirty)"
2024-12-19 17:46:09 time=2024-12-20T01:46:09.828Z level=INFO source=routes.go:1339 msg="Dynamic LLM libraries" runners="[cuda_v12_avx cpu cpu_avx cpu_avx2 cuda_v11_avx]"
2024-12-19 17:46:09 time=2024-12-20T01:46:09.828Z level=INFO source=gpu.go:226 msg="looking for compatible GPUs"
2024-12-19 17:46:10 time=2024-12-20T01:46:10.376Z level=INFO source=types.go:131 msg="inference compute" id=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce library=cuda variant=v12 compute=8.9 driver=12.6 name="NVIDIA GeForce RTX 4070" total="12.0 GiB" available="10.8 GiB"
2024-12-19 17:46:09 [GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)
2024-12-19 17:46:09 [GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)
2024-12-19 17:46:09 [GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)
2024-12-19 17:46:09 [GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)
2024-12-19 17:46:09 [GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)
2024-12-19 17:46:09 [GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)
2024-12-19 17:46:09 [GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)
2024-12-19 17:46:09 [GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)
2024-12-19 17:46:09 [GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)
2024-12-19 17:46:28 [GIN] 2024/12/20 - 01:46:28 | 200 |      30.175µs |       127.0.0.1 | HEAD     "/"
2024-12-19 17:46:28 [GIN] 2024/12/20 - 01:46:28 | 200 |  143.386111ms |       127.0.0.1 | POST     "/api/show"
2024-12-19 17:46:10 time=2024-12-20T01:46:10.376Z level=INFO source=types.go:131 msg="inference compute" id=GPU-bed8562d-fb5d-389f-72d0-fcc5fb8676d9 library=cuda variant=v12 compute=7.5 driver=12.6 name="NVIDIA GeForce GTX 1650" total="4.0 GiB" available="3.2 GiB"
2024-12-19 17:46:28 time=2024-12-20T01:46:28.614Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=11632902144 required="8.7 GiB"
2024-12-19 17:46:28 time=2024-12-20T01:46:28.902Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="29.3 GiB" free_swap="8.0 GiB"
2024-12-19 17:46:28 time=2024-12-20T01:46:28.902Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[10.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="8.7 GiB" memory.required.kv="4.0 GiB" memory.required.allocations="[8.7 GiB]" memory.weights.total="7.4 GiB" memory.weights.repeating="7.3 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="681.0 MiB"
2024-12-19 17:46:28 time=2024-12-20T01:46:28.903Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --parallel 4 --port 34261"
2024-12-19 17:46:28 time=2024-12-20T01:46:28.903Z level=INFO source=sched.go:449 msg="loaded runners" count=1
2024-12-19 17:46:28 time=2024-12-20T01:46:28.903Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 17:46:28 time=2024-12-20T01:46:28.904Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 17:46:28 time=2024-12-20T01:46:28.926Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 17:46:29 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 17:46:29 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 17:46:29 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 17:46:29   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 17:46:29 time=2024-12-20T01:46:29.009Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=8
2024-12-19 17:46:29 time=2024-12-20T01:46:29.009Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:34261"
2024-12-19 17:46:29 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 17:46:29 time=2024-12-20T01:46:29.155Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 17:46:29 llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))
2024-12-19 17:46:29 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 17:46:29 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 17:46:29 llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
2024-12-19 17:46:29 llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
2024-12-19 17:46:29 llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
2024-12-19 17:46:29 llama_model_loader: - kv   4:                          llama.block_count u32              = 32
2024-12-19 17:46:29 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
2024-12-19 17:46:29 llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
2024-12-19 17:46:29 llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
2024-12-19 17:46:29 llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
2024-12-19 17:46:29 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 17:46:29 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 17:46:29 llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
2024-12-19 17:46:29 llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
2024-12-19 17:46:29 llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
2024-12-19 17:46:29 llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
2024-12-19 17:46:29 llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
2024-12-19 17:46:29 llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
2024-12-19 17:46:29 llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
2024-12-19 17:46:29 llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
2024-12-19 17:46:29 llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
2024-12-19 17:46:29 llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
2024-12-19 17:46:29 llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...
2024-12-19 17:46:29 llama_model_loader: - kv  22:               general.quantization_version u32              = 2
2024-12-19 17:46:29 llama_model_loader: - type  f32:   65 tensors
2024-12-19 17:46:29 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 17:46:29 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 17:46:29 llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
2024-12-19 17:46:29 llm_load_vocab: special tokens cache size = 3
2024-12-19 17:46:29 llm_load_vocab: token to piece cache size = 0.1684 MB
2024-12-19 17:46:29 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 17:46:29 llm_load_print_meta: arch             = llama
2024-12-19 17:46:29 llm_load_print_meta: vocab type       = SPM
2024-12-19 17:46:29 llm_load_print_meta: n_vocab          = 32000
2024-12-19 17:46:29 llm_load_print_meta: n_merges         = 0
2024-12-19 17:46:29 llm_load_print_meta: vocab_only       = 0
2024-12-19 17:46:29 llm_load_print_meta: n_ctx_train      = 4096
2024-12-19 17:46:29 llm_load_print_meta: n_embd           = 4096
2024-12-19 17:46:29 llm_load_print_meta: n_layer          = 32
2024-12-19 17:46:29 llm_load_print_meta: n_head           = 32
2024-12-19 17:46:29 llm_load_print_meta: n_head_kv        = 32
2024-12-19 17:46:29 llm_load_print_meta: n_rot            = 128
2024-12-19 17:46:29 llm_load_print_meta: n_swa            = 0
2024-12-19 17:46:29 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 17:46:29 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 17:46:29 llm_load_print_meta: n_gqa            = 1
2024-12-19 17:46:29 llm_load_print_meta: n_embd_k_gqa     = 4096
2024-12-19 17:46:29 llm_load_print_meta: n_embd_v_gqa     = 4096
2024-12-19 17:46:29 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 17:46:29 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 17:46:29 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 17:46:29 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 17:46:29 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 17:46:29 llm_load_print_meta: n_ff             = 11008
2024-12-19 17:46:29 llm_load_print_meta: n_expert         = 0
2024-12-19 17:46:29 llm_load_print_meta: n_expert_used    = 0
2024-12-19 17:46:29 llm_load_print_meta: causal attn      = 1
2024-12-19 17:46:29 llm_load_print_meta: pooling type     = 0
2024-12-19 17:46:29 llm_load_print_meta: rope type        = 0
2024-12-19 17:46:29 llm_load_print_meta: rope scaling     = linear
2024-12-19 17:46:29 llm_load_print_meta: freq_base_train  = 10000.0
2024-12-19 17:46:29 llm_load_print_meta: freq_scale_train = 1
2024-12-19 17:46:29 llm_load_print_meta: n_ctx_orig_yarn  = 4096
2024-12-19 17:46:29 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 17:46:29 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 17:46:29 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 17:46:29 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 17:46:29 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 17:46:29 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 17:46:29 llm_load_print_meta: model type       = 7B
2024-12-19 17:46:29 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 17:46:29 llm_load_print_meta: model params     = 6.74 B
2024-12-19 17:46:29 llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
2024-12-19 17:46:29 llm_load_print_meta: general.name     = LLaMA v2
2024-12-19 17:46:29 llm_load_print_meta: BOS token        = 1 '<s>'
2024-12-19 17:46:29 llm_load_print_meta: EOS token        = 2 '</s>'
2024-12-19 17:46:29 llm_load_print_meta: UNK token        = 0 '<unk>'
2024-12-19 17:46:29 llm_load_print_meta: LF token         = 13 '<0x0A>'
2024-12-19 17:46:29 llm_load_print_meta: EOG token        = 2 '</s>'
2024-12-19 17:46:29 llm_load_print_meta: max token length = 48
2024-12-19 17:46:39 time=2024-12-20T01:46:39.505Z level=WARN source=server.go:562 msg="client connection closed before server finished loading, aborting load"
2024-12-19 17:46:39 time=2024-12-20T01:46:39.505Z level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
2024-12-19 17:46:44 time=2024-12-20T01:46:44.727Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.222157015 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 17:46:45 time=2024-12-20T01:46:45.004Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.498848249 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 17:46:45 time=2024-12-20T01:46:45.284Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.778819619 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 17:46:53 time=2024-12-20T01:46:53.942Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=11632902144 required="6.2 GiB"
2024-12-19 17:46:54 time=2024-12-20T01:46:54.228Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="29.3 GiB" free_swap="8.0 GiB"
2024-12-19 17:46:54 time=2024-12-20T01:46:54.229Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[10.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.7 GiB" memory.weights.repeating="4.3 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
2024-12-19 17:46:54 time=2024-12-20T01:46:54.230Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --parallel 4 --port 43587"
2024-12-19 17:46:54 time=2024-12-20T01:46:54.230Z level=INFO source=sched.go:449 msg="loaded runners" count=1
2024-12-19 17:46:54 time=2024-12-20T01:46:54.230Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 17:46:54 time=2024-12-20T01:46:54.230Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 17:46:54 time=2024-12-20T01:46:54.252Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 17:46:54 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 17:46:54 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 17:46:54 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 17:46:54   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 17:46:54 time=2024-12-20T01:46:54.329Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=8
2024-12-19 17:46:54 time=2024-12-20T01:46:54.329Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:43587"
2024-12-19 17:46:54 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 17:46:54 time=2024-12-20T01:46:54.481Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 17:46:54 llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
2024-12-19 17:46:54 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 17:46:54 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 17:46:54 llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
2024-12-19 17:46:54 llama_model_loader: - kv   2:                          llama.block_count u32              = 32
2024-12-19 17:46:54 llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
2024-12-19 17:46:54 llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
2024-12-19 17:46:54 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
2024-12-19 17:46:54 llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
2024-12-19 17:46:54 llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
2024-12-19 17:46:54 llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
2024-12-19 17:46:54 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 17:46:54 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 17:46:54 llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
2024-12-19 17:46:54 llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
2024-12-19 17:46:54 llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
2024-12-19 17:46:54 llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
2024-12-19 17:46:54 llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2024-12-19 17:46:54 llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2024-12-19 17:46:54 llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
2024-12-19 17:46:54 llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
2024-12-19 17:46:54 llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
2024-12-19 17:46:54 llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
2024-12-19 17:46:54 llama_model_loader: - kv  21:               general.quantization_version u32              = 2
2024-12-19 17:46:54 llama_model_loader: - type  f32:   65 tensors
2024-12-19 17:46:54 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 17:46:54 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 17:46:55 llm_load_vocab: special tokens cache size = 256
2024-12-19 17:46:55 llm_load_vocab: token to piece cache size = 0.8000 MB
2024-12-19 17:46:55 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 17:46:55 llm_load_print_meta: arch             = llama
2024-12-19 17:46:55 llm_load_print_meta: vocab type       = BPE
2024-12-19 17:46:55 llm_load_print_meta: n_vocab          = 128256
2024-12-19 17:46:55 llm_load_print_meta: n_merges         = 280147
2024-12-19 17:46:55 llm_load_print_meta: vocab_only       = 0
2024-12-19 17:46:55 llm_load_print_meta: n_ctx_train      = 8192
2024-12-19 17:46:55 llm_load_print_meta: n_embd           = 4096
2024-12-19 17:46:55 llm_load_print_meta: n_layer          = 32
2024-12-19 17:46:55 llm_load_print_meta: n_head           = 32
2024-12-19 17:46:55 llm_load_print_meta: n_head_kv        = 8
2024-12-19 17:46:55 llm_load_print_meta: n_rot            = 128
2024-12-19 17:46:55 llm_load_print_meta: n_swa            = 0
2024-12-19 17:46:55 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 17:46:55 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 17:46:55 llm_load_print_meta: n_gqa            = 4
2024-12-19 17:46:55 llm_load_print_meta: n_embd_k_gqa     = 1024
2024-12-19 17:46:55 llm_load_print_meta: n_embd_v_gqa     = 1024
2024-12-19 17:46:55 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 17:46:55 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 17:46:55 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 17:46:55 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 17:46:55 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 17:46:55 llm_load_print_meta: n_ff             = 14336
2024-12-19 17:46:55 llm_load_print_meta: n_expert         = 0
2024-12-19 17:46:55 llm_load_print_meta: n_expert_used    = 0
2024-12-19 17:46:55 llm_load_print_meta: causal attn      = 1
2024-12-19 17:46:55 llm_load_print_meta: pooling type     = 0
2024-12-19 17:46:55 llm_load_print_meta: rope type        = 0
2024-12-19 17:46:55 llm_load_print_meta: rope scaling     = linear
2024-12-19 17:46:55 llm_load_print_meta: freq_base_train  = 500000.0
2024-12-19 17:46:55 llm_load_print_meta: freq_scale_train = 1
2024-12-19 17:46:55 llm_load_print_meta: n_ctx_orig_yarn  = 8192
2024-12-19 17:46:55 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 17:46:55 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 17:46:55 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 17:46:55 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 17:46:55 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 17:46:55 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 17:46:55 llm_load_print_meta: model type       = 8B
2024-12-19 17:46:55 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 17:46:55 llm_load_print_meta: model params     = 8.03 B
2024-12-19 17:46:55 llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
2024-12-19 17:46:55 llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
2024-12-19 17:46:55 llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
2024-12-19 17:46:55 llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
2024-12-19 17:46:55 llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
2024-12-19 17:46:55 llm_load_print_meta: LF token         = 128 'Ä'
2024-12-19 17:46:55 llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
2024-12-19 17:46:55 llm_load_print_meta: max token length = 256
2024-12-19 17:49:42 llm_load_tensors: offloading 32 repeating layers to GPU
2024-12-19 17:49:42 llm_load_tensors: offloading output layer to GPU
2024-12-19 17:49:42 llm_load_tensors: offloaded 33/33 layers to GPU
2024-12-19 17:49:42 llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB
2024-12-19 17:49:42 llm_load_tensors:        CUDA0 model buffer size =  4155.99 MiB
2024-12-19 17:49:43 llama_new_context_with_model: n_seq_max     = 4
2024-12-19 17:49:43 llama_new_context_with_model: n_ctx         = 8192
2024-12-19 17:49:43 llama_new_context_with_model: n_ctx_per_seq = 2048
2024-12-19 17:49:43 llama_new_context_with_model: n_batch       = 2048
2024-12-19 17:49:43 llama_new_context_with_model: n_ubatch      = 512
2024-12-19 17:49:43 llama_new_context_with_model: flash_attn    = 0
2024-12-19 17:49:43 llama_new_context_with_model: freq_base     = 500000.0
2024-12-19 17:49:43 llama_new_context_with_model: freq_scale    = 1
2024-12-19 17:49:43 llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
2024-12-19 17:49:43 llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB
2024-12-19 17:49:43 llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB
2024-12-19 17:49:43 llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB
2024-12-19 17:49:43 llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB
2024-12-19 17:49:43 llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB
2024-12-19 17:49:43 llama_new_context_with_model: graph nodes  = 1030
2024-12-19 17:49:43 llama_new_context_with_model: graph splits = 2
2024-12-19 17:49:43 time=2024-12-20T01:49:43.401Z level=INFO source=server.go:594 msg="llama runner started in 158.77 seconds"
2024-12-19 17:51:09 llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
2024-12-19 17:51:09 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 17:51:09 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 17:51:09 llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
2024-12-19 17:51:09 llama_model_loader: - kv   2:                          llama.block_count u32              = 32
2024-12-19 17:51:09 llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
2024-12-19 17:51:09 llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
2024-12-19 17:51:09 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
2024-12-19 17:51:09 llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
2024-12-19 17:51:09 llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
2024-12-19 17:51:09 llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
2024-12-19 17:51:09 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 17:51:09 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 17:51:09 llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
2024-12-19 17:51:09 llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
2024-12-19 17:51:09 llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
2024-12-19 17:51:09 llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
2024-12-19 17:51:09 llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2024-12-19 17:51:09 llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2024-12-19 17:51:09 llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
2024-12-19 17:51:09 llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
2024-12-19 17:51:09 llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
2024-12-19 17:51:09 llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
2024-12-19 17:51:09 llama_model_loader: - kv  21:               general.quantization_version u32              = 2
2024-12-19 17:51:09 llama_model_loader: - type  f32:   65 tensors
2024-12-19 17:51:09 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 17:51:09 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 17:51:09 llm_load_vocab: special tokens cache size = 256
2024-12-19 17:51:09 llm_load_vocab: token to piece cache size = 0.8000 MB
2024-12-19 17:51:09 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 17:51:09 llm_load_print_meta: arch             = llama
2024-12-19 17:51:09 llm_load_print_meta: vocab type       = BPE
2024-12-19 17:51:09 llm_load_print_meta: n_vocab          = 128256
2024-12-19 17:51:09 llm_load_print_meta: n_merges         = 280147
2024-12-19 17:51:09 llm_load_print_meta: vocab_only       = 1
2024-12-19 17:51:09 llm_load_print_meta: model type       = ?B
2024-12-19 17:51:09 llm_load_print_meta: model ftype      = all F32
2024-12-19 17:51:09 llm_load_print_meta: model params     = 8.03 B
2024-12-19 17:51:09 llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
2024-12-19 17:51:09 llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
2024-12-19 17:51:09 llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
2024-12-19 17:51:09 llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
2024-12-19 17:51:09 llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
2024-12-19 17:51:09 llm_load_print_meta: LF token         = 128 'Ä'
2024-12-19 17:51:09 llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
2024-12-19 17:51:09 llm_load_print_meta: max token length = 256
2024-12-19 17:51:09 llama_model_load: vocab only - skipping tensors
2024-12-19 17:55:25 time=2024-12-20T01:55:25.623Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce library=cuda total="12.0 GiB" available="5.8 GiB"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.623Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-bed8562d-fb5d-389f-72d0-fcc5fb8676d9 library=cuda total="4.0 GiB" available="3.2 GiB"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.623Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=6223796224 required="1.6 GiB"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.862Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="28.3 GiB" free_swap="8.0 GiB"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.862Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=1 layers.model=33 layers.offload=1 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="1.6 GiB" memory.required.kv="4.0 GiB" memory.required.allocations="[1.6 GiB]" memory.weights.total="7.4 GiB" memory.weights.repeating="7.3 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="681.0 MiB"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.866Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 8192 --batch-size 512 --n-gpu-layers 1 --threads 4 --parallel 4 --port 39871"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.866Z level=INFO source=sched.go:449 msg="loaded runners" count=2
2024-12-19 17:55:25 time=2024-12-20T01:55:25.866Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.867Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 17:55:25 time=2024-12-20T01:55:25.929Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 17:55:26 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 17:55:26 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 17:55:26 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 17:55:26   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 17:55:26 time=2024-12-20T01:55:26.002Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=4
2024-12-19 17:55:26 time=2024-12-20T01:55:26.002Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:39871"
2024-12-19 17:55:26 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 17:55:26 time=2024-12-20T01:55:26.118Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 17:55:26 llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))
2024-12-19 17:55:26 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 17:55:26 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 17:55:26 llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
2024-12-19 17:55:26 llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
2024-12-19 17:55:26 llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
2024-12-19 17:55:26 llama_model_loader: - kv   4:                          llama.block_count u32              = 32
2024-12-19 17:55:26 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
2024-12-19 17:55:26 llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
2024-12-19 17:55:26 llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
2024-12-19 17:55:26 llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
2024-12-19 17:55:26 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 17:55:26 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 17:55:26 llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
2024-12-19 17:55:26 llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
2024-12-19 17:55:26 llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
2024-12-19 17:55:26 llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
2024-12-19 17:55:26 llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
2024-12-19 17:55:26 llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
2024-12-19 17:55:26 llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
2024-12-19 17:55:26 llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
2024-12-19 17:55:26 llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
2024-12-19 17:55:26 llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
2024-12-19 17:55:26 llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...
2024-12-19 17:55:26 llama_model_loader: - kv  22:               general.quantization_version u32              = 2
2024-12-19 17:55:26 llama_model_loader: - type  f32:   65 tensors
2024-12-19 17:55:26 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 17:55:26 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 17:55:26 llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
2024-12-19 17:55:26 llm_load_vocab: special tokens cache size = 3
2024-12-19 17:55:26 llm_load_vocab: token to piece cache size = 0.1684 MB
2024-12-19 17:55:26 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 17:55:26 llm_load_print_meta: arch             = llama
2024-12-19 17:55:26 llm_load_print_meta: vocab type       = SPM
2024-12-19 17:55:26 llm_load_print_meta: n_vocab          = 32000
2024-12-19 17:55:26 llm_load_print_meta: n_merges         = 0
2024-12-19 17:55:26 llm_load_print_meta: vocab_only       = 0
2024-12-19 17:55:26 llm_load_print_meta: n_ctx_train      = 4096
2024-12-19 17:55:26 llm_load_print_meta: n_embd           = 4096
2024-12-19 17:55:26 llm_load_print_meta: n_layer          = 32
2024-12-19 17:55:26 llm_load_print_meta: n_head           = 32
2024-12-19 17:55:26 llm_load_print_meta: n_head_kv        = 32
2024-12-19 17:55:26 llm_load_print_meta: n_rot            = 128
2024-12-19 17:55:26 llm_load_print_meta: n_swa            = 0
2024-12-19 17:55:26 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 17:55:26 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 17:55:26 llm_load_print_meta: n_gqa            = 1
2024-12-19 17:55:26 llm_load_print_meta: n_embd_k_gqa     = 4096
2024-12-19 17:55:26 llm_load_print_meta: n_embd_v_gqa     = 4096
2024-12-19 17:55:26 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 17:55:26 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 17:55:26 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 17:55:26 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 17:55:26 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 17:55:26 llm_load_print_meta: n_ff             = 11008
2024-12-19 17:55:26 llm_load_print_meta: n_expert         = 0
2024-12-19 17:55:26 llm_load_print_meta: n_expert_used    = 0
2024-12-19 17:55:26 llm_load_print_meta: causal attn      = 1
2024-12-19 17:55:26 llm_load_print_meta: pooling type     = 0
2024-12-19 17:55:26 llm_load_print_meta: rope type        = 0
2024-12-19 17:55:26 llm_load_print_meta: rope scaling     = linear
2024-12-19 17:55:26 llm_load_print_meta: freq_base_train  = 10000.0
2024-12-19 17:55:26 llm_load_print_meta: freq_scale_train = 1
2024-12-19 17:55:26 llm_load_print_meta: n_ctx_orig_yarn  = 4096
2024-12-19 17:55:26 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 17:55:26 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 17:55:26 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 17:55:26 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 17:55:26 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 17:55:26 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 17:55:26 llm_load_print_meta: model type       = 7B
2024-12-19 17:55:26 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 17:55:26 llm_load_print_meta: model params     = 6.74 B
2024-12-19 17:55:26 llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
2024-12-19 17:55:26 llm_load_print_meta: general.name     = LLaMA v2
2024-12-19 17:55:26 llm_load_print_meta: BOS token        = 1 '<s>'
2024-12-19 17:55:26 llm_load_print_meta: EOS token        = 2 '</s>'
2024-12-19 17:55:26 llm_load_print_meta: UNK token        = 0 '<unk>'
2024-12-19 17:55:26 llm_load_print_meta: LF token         = 13 '<0x0A>'
2024-12-19 17:55:26 llm_load_print_meta: EOG token        = 2 '</s>'
2024-12-19 17:55:26 llm_load_print_meta: max token length = 48
2024-12-19 17:57:33 time=2024-12-20T01:57:33.566Z level=WARN source=server.go:562 msg="client connection closed before server finished loading, aborting load"
2024-12-19 17:57:33 time=2024-12-20T01:57:33.567Z level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
2024-12-19 17:57:38 time=2024-12-20T01:57:38.845Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.278638536 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 17:57:39 time=2024-12-20T01:57:39.129Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.562245412 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 17:57:39 time=2024-12-20T01:57:39.386Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.8191423570000005 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:02:21 time=2024-12-20T02:02:21.287Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce library=cuda total="12.0 GiB" available="5.8 GiB"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.287Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-bed8562d-fb5d-389f-72d0-fcc5fb8676d9 library=cuda total="4.0 GiB" available="3.2 GiB"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.287Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=6223796224 required="1.6 GiB"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.539Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="28.3 GiB" free_swap="8.0 GiB"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.539Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=1 layers.model=33 layers.offload=1 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="1.6 GiB" memory.required.kv="4.0 GiB" memory.required.allocations="[1.6 GiB]" memory.weights.total="7.4 GiB" memory.weights.repeating="7.3 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="681.0 MiB"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.541Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 8192 --batch-size 512 --n-gpu-layers 1 --threads 4 --parallel 4 --port 39817"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.541Z level=INFO source=sched.go:449 msg="loaded runners" count=2
2024-12-19 18:02:21 time=2024-12-20T02:02:21.541Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.541Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.608Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 18:02:21 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 18:02:21 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 18:02:21 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 18:02:21   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 18:02:21 time=2024-12-20T02:02:21.707Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=4
2024-12-19 18:02:21 time=2024-12-20T02:02:21.707Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:39817"
2024-12-19 18:02:21 time=2024-12-20T02:02:21.793Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 18:02:21 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 18:02:21 llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))
2024-12-19 18:02:21 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 18:02:21 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 18:02:21 llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
2024-12-19 18:02:21 llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
2024-12-19 18:02:21 llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
2024-12-19 18:02:21 llama_model_loader: - kv   4:                          llama.block_count u32              = 32
2024-12-19 18:02:21 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
2024-12-19 18:02:21 llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
2024-12-19 18:02:21 llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
2024-12-19 18:02:21 llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
2024-12-19 18:02:21 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 18:02:21 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 18:02:21 llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
2024-12-19 18:02:21 llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
2024-12-19 18:02:21 llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
2024-12-19 18:02:21 llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
2024-12-19 18:02:21 llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
2024-12-19 18:02:21 llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
2024-12-19 18:02:21 llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
2024-12-19 18:02:21 llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
2024-12-19 18:02:21 llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
2024-12-19 18:02:21 llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
2024-12-19 18:02:21 llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...
2024-12-19 18:02:21 llama_model_loader: - kv  22:               general.quantization_version u32              = 2
2024-12-19 18:02:21 llama_model_loader: - type  f32:   65 tensors
2024-12-19 18:02:21 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 18:02:21 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 18:02:21 llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
2024-12-19 18:02:21 llm_load_vocab: special tokens cache size = 3
2024-12-19 18:02:21 llm_load_vocab: token to piece cache size = 0.1684 MB
2024-12-19 18:02:21 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 18:02:21 llm_load_print_meta: arch             = llama
2024-12-19 18:02:21 llm_load_print_meta: vocab type       = SPM
2024-12-19 18:02:21 llm_load_print_meta: n_vocab          = 32000
2024-12-19 18:02:21 llm_load_print_meta: n_merges         = 0
2024-12-19 18:02:21 llm_load_print_meta: vocab_only       = 0
2024-12-19 18:02:21 llm_load_print_meta: n_ctx_train      = 4096
2024-12-19 18:02:21 llm_load_print_meta: n_embd           = 4096
2024-12-19 18:02:21 llm_load_print_meta: n_layer          = 32
2024-12-19 18:02:21 llm_load_print_meta: n_head           = 32
2024-12-19 18:02:21 llm_load_print_meta: n_head_kv        = 32
2024-12-19 18:02:21 llm_load_print_meta: n_rot            = 128
2024-12-19 18:02:21 llm_load_print_meta: n_swa            = 0
2024-12-19 18:02:21 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 18:02:21 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 18:02:21 llm_load_print_meta: n_gqa            = 1
2024-12-19 18:02:21 llm_load_print_meta: n_embd_k_gqa     = 4096
2024-12-19 18:02:21 llm_load_print_meta: n_embd_v_gqa     = 4096
2024-12-19 18:02:21 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 18:02:21 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 18:02:21 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 18:02:21 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 18:02:21 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 18:02:21 llm_load_print_meta: n_ff             = 11008
2024-12-19 18:02:21 llm_load_print_meta: n_expert         = 0
2024-12-19 18:02:21 llm_load_print_meta: n_expert_used    = 0
2024-12-19 18:02:21 llm_load_print_meta: causal attn      = 1
2024-12-19 18:02:21 llm_load_print_meta: pooling type     = 0
2024-12-19 18:02:21 llm_load_print_meta: rope type        = 0
2024-12-19 18:02:21 llm_load_print_meta: rope scaling     = linear
2024-12-19 18:02:21 llm_load_print_meta: freq_base_train  = 10000.0
2024-12-19 18:02:21 llm_load_print_meta: freq_scale_train = 1
2024-12-19 18:02:21 llm_load_print_meta: n_ctx_orig_yarn  = 4096
2024-12-19 18:02:21 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 18:02:21 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 18:02:21 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 18:02:21 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 18:02:21 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 18:02:21 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 18:02:21 llm_load_print_meta: model type       = 7B
2024-12-19 18:02:21 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 18:02:21 llm_load_print_meta: model params     = 6.74 B
2024-12-19 18:02:21 llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
2024-12-19 18:02:21 llm_load_print_meta: general.name     = LLaMA v2
2024-12-19 18:02:21 llm_load_print_meta: BOS token        = 1 '<s>'
2024-12-19 18:02:21 llm_load_print_meta: EOS token        = 2 '</s>'
2024-12-19 18:02:21 llm_load_print_meta: UNK token        = 0 '<unk>'
2024-12-19 18:02:21 llm_load_print_meta: LF token         = 13 '<0x0A>'
2024-12-19 18:02:21 llm_load_print_meta: EOG token        = 2 '</s>'
2024-12-19 18:02:21 llm_load_print_meta: max token length = 48
2024-12-19 18:04:29 time=2024-12-20T02:04:29.198Z level=WARN source=server.go:562 msg="client connection closed before server finished loading, aborting load"
2024-12-19 18:04:29 time=2024-12-20T02:04:29.199Z level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
2024-12-19 18:04:34 time=2024-12-20T02:04:34.417Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.218678713 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:04:34 time=2024-12-20T02:04:34.698Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.499315124 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:04:34 time=2024-12-20T02:04:34.959Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.760755596 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:11:02 time=2024-12-20T02:11:02.695Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce library=cuda total="12.0 GiB" available="5.8 GiB"
2024-12-19 18:11:02 time=2024-12-20T02:11:02.695Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-bed8562d-fb5d-389f-72d0-fcc5fb8676d9 library=cuda total="4.0 GiB" available="3.2 GiB"
2024-12-19 18:11:02 time=2024-12-20T02:11:02.696Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=6223796224 required="1.6 GiB"
2024-12-19 18:11:02 time=2024-12-20T02:11:02.960Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="28.3 GiB" free_swap="8.0 GiB"
2024-12-19 18:11:02 time=2024-12-20T02:11:02.960Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=1 layers.model=33 layers.offload=1 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="1.6 GiB" memory.required.kv="4.0 GiB" memory.required.allocations="[1.6 GiB]" memory.weights.total="7.4 GiB" memory.weights.repeating="7.3 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="681.0 MiB"
2024-12-19 18:11:02 time=2024-12-20T02:11:02.962Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 8192 --batch-size 512 --n-gpu-layers 1 --threads 4 --parallel 4 --port 36365"
2024-12-19 18:11:02 time=2024-12-20T02:11:02.962Z level=INFO source=sched.go:449 msg="loaded runners" count=2
2024-12-19 18:11:02 time=2024-12-20T02:11:02.963Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 18:11:02 time=2024-12-20T02:11:02.963Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 18:11:03 time=2024-12-20T02:11:03.026Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 18:11:03 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 18:11:03 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 18:11:03 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 18:11:03   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 18:11:03 time=2024-12-20T02:11:03.131Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=4
2024-12-19 18:11:03 time=2024-12-20T02:11:03.131Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:36365"
2024-12-19 18:11:03 time=2024-12-20T02:11:03.215Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 18:11:03 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 18:11:03 llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))
2024-12-19 18:11:03 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 18:11:03 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 18:11:03 llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
2024-12-19 18:11:03 llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
2024-12-19 18:11:03 llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
2024-12-19 18:11:03 llama_model_loader: - kv   4:                          llama.block_count u32              = 32
2024-12-19 18:11:03 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
2024-12-19 18:11:03 llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
2024-12-19 18:11:03 llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
2024-12-19 18:11:03 llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
2024-12-19 18:11:03 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 18:11:03 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 18:11:03 llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
2024-12-19 18:11:03 llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
2024-12-19 18:11:03 llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
2024-12-19 18:11:03 llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
2024-12-19 18:11:03 llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
2024-12-19 18:11:03 llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
2024-12-19 18:11:03 llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
2024-12-19 18:11:03 llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
2024-12-19 18:11:03 llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
2024-12-19 18:11:03 llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
2024-12-19 18:11:03 llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...
2024-12-19 18:11:03 llama_model_loader: - kv  22:               general.quantization_version u32              = 2
2024-12-19 18:11:03 llama_model_loader: - type  f32:   65 tensors
2024-12-19 18:11:03 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 18:11:03 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 18:11:03 llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
2024-12-19 18:11:03 llm_load_vocab: special tokens cache size = 3
2024-12-19 18:11:03 llm_load_vocab: token to piece cache size = 0.1684 MB
2024-12-19 18:11:03 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 18:11:03 llm_load_print_meta: arch             = llama
2024-12-19 18:11:03 llm_load_print_meta: vocab type       = SPM
2024-12-19 18:11:03 llm_load_print_meta: n_vocab          = 32000
2024-12-19 18:11:03 llm_load_print_meta: n_merges         = 0
2024-12-19 18:11:03 llm_load_print_meta: vocab_only       = 0
2024-12-19 18:11:03 llm_load_print_meta: n_ctx_train      = 4096
2024-12-19 18:11:03 llm_load_print_meta: n_embd           = 4096
2024-12-19 18:11:03 llm_load_print_meta: n_layer          = 32
2024-12-19 18:11:03 llm_load_print_meta: n_head           = 32
2024-12-19 18:11:03 llm_load_print_meta: n_head_kv        = 32
2024-12-19 18:11:03 llm_load_print_meta: n_rot            = 128
2024-12-19 18:11:03 llm_load_print_meta: n_swa            = 0
2024-12-19 18:11:03 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 18:11:03 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 18:11:03 llm_load_print_meta: n_gqa            = 1
2024-12-19 18:11:03 llm_load_print_meta: n_embd_k_gqa     = 4096
2024-12-19 18:11:03 llm_load_print_meta: n_embd_v_gqa     = 4096
2024-12-19 18:11:03 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 18:11:03 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 18:11:03 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 18:11:03 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 18:11:03 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 18:11:03 llm_load_print_meta: n_ff             = 11008
2024-12-19 18:11:03 llm_load_print_meta: n_expert         = 0
2024-12-19 18:11:03 llm_load_print_meta: n_expert_used    = 0
2024-12-19 18:11:03 llm_load_print_meta: causal attn      = 1
2024-12-19 18:11:03 llm_load_print_meta: pooling type     = 0
2024-12-19 18:11:03 llm_load_print_meta: rope type        = 0
2024-12-19 18:11:03 llm_load_print_meta: rope scaling     = linear
2024-12-19 18:11:03 llm_load_print_meta: freq_base_train  = 10000.0
2024-12-19 18:11:03 llm_load_print_meta: freq_scale_train = 1
2024-12-19 18:11:03 llm_load_print_meta: n_ctx_orig_yarn  = 4096
2024-12-19 18:11:03 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 18:11:03 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 18:11:03 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 18:11:03 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 18:11:03 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 18:11:03 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 18:11:03 llm_load_print_meta: model type       = 7B
2024-12-19 18:11:03 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 18:11:03 llm_load_print_meta: model params     = 6.74 B
2024-12-19 18:11:03 llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
2024-12-19 18:11:03 llm_load_print_meta: general.name     = LLaMA v2
2024-12-19 18:11:03 llm_load_print_meta: BOS token        = 1 '<s>'
2024-12-19 18:11:03 llm_load_print_meta: EOS token        = 2 '</s>'
2024-12-19 18:11:03 llm_load_print_meta: UNK token        = 0 '<unk>'
2024-12-19 18:11:03 llm_load_print_meta: LF token         = 13 '<0x0A>'
2024-12-19 18:11:03 llm_load_print_meta: EOG token        = 2 '</s>'
2024-12-19 18:11:03 llm_load_print_meta: max token length = 48
2024-12-19 18:13:10 time=2024-12-20T02:13:10.681Z level=WARN source=server.go:562 msg="client connection closed before server finished loading, aborting load"
2024-12-19 18:13:10 time=2024-12-20T02:13:10.681Z level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
2024-12-19 18:13:15 time=2024-12-20T02:13:15.931Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.25049876 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:13:16 time=2024-12-20T02:13:16.196Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.51507658 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:13:16 time=2024-12-20T02:13:16.455Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.7736941680000005 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:18:09 time=2024-12-20T02:18:09.872Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce library=cuda total="12.0 GiB" available="5.8 GiB"
2024-12-19 18:18:09 time=2024-12-20T02:18:09.872Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-bed8562d-fb5d-389f-72d0-fcc5fb8676d9 library=cuda total="4.0 GiB" available="3.2 GiB"
2024-12-19 18:18:09 time=2024-12-20T02:18:09.872Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=6223796224 required="1.6 GiB"
2024-12-19 18:18:10 time=2024-12-20T02:18:10.124Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="28.3 GiB" free_swap="8.0 GiB"
2024-12-19 18:18:10 time=2024-12-20T02:18:10.124Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=1 layers.model=33 layers.offload=1 layers.split="" memory.available="[5.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="1.6 GiB" memory.required.kv="4.0 GiB" memory.required.allocations="[1.6 GiB]" memory.weights.total="7.4 GiB" memory.weights.repeating="7.3 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="681.0 MiB"
2024-12-19 18:18:10 time=2024-12-20T02:18:10.124Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 8192 --batch-size 512 --n-gpu-layers 1 --threads 4 --parallel 4 --port 34063"
2024-12-19 18:18:10 time=2024-12-20T02:18:10.125Z level=INFO source=sched.go:449 msg="loaded runners" count=2
2024-12-19 18:18:10 time=2024-12-20T02:18:10.125Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 18:18:10 time=2024-12-20T02:18:10.125Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 18:18:10 time=2024-12-20T02:18:10.148Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 18:18:10 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 18:18:10 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 18:18:10 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 18:18:10   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 18:18:10 time=2024-12-20T02:18:10.228Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=4
2024-12-19 18:18:10 time=2024-12-20T02:18:10.228Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:34063"
2024-12-19 18:18:10 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 18:18:10 llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))
2024-12-19 18:18:10 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 18:18:10 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 18:18:10 llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
2024-12-19 18:18:10 llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
2024-12-19 18:18:10 llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
2024-12-19 18:18:10 llama_model_loader: - kv   4:                          llama.block_count u32              = 32
2024-12-19 18:18:10 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
2024-12-19 18:18:10 llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
2024-12-19 18:18:10 llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
2024-12-19 18:18:10 llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
2024-12-19 18:18:10 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 18:18:10 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 18:18:10 llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
2024-12-19 18:18:10 time=2024-12-20T02:18:10.376Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 18:18:10 llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
2024-12-19 18:18:10 llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
2024-12-19 18:18:10 llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
2024-12-19 18:18:10 llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
2024-12-19 18:18:10 llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
2024-12-19 18:18:10 llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
2024-12-19 18:18:10 llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
2024-12-19 18:18:10 llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
2024-12-19 18:18:10 llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
2024-12-19 18:18:10 llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...
2024-12-19 18:18:10 llama_model_loader: - kv  22:               general.quantization_version u32              = 2
2024-12-19 18:18:10 llama_model_loader: - type  f32:   65 tensors
2024-12-19 18:18:10 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 18:18:10 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 18:18:10 llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
2024-12-19 18:18:10 llm_load_vocab: special tokens cache size = 3
2024-12-19 18:18:10 llm_load_vocab: token to piece cache size = 0.1684 MB
2024-12-19 18:18:10 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 18:18:10 llm_load_print_meta: arch             = llama
2024-12-19 18:18:10 llm_load_print_meta: vocab type       = SPM
2024-12-19 18:18:10 llm_load_print_meta: n_vocab          = 32000
2024-12-19 18:18:10 llm_load_print_meta: n_merges         = 0
2024-12-19 18:18:10 llm_load_print_meta: vocab_only       = 0
2024-12-19 18:18:10 llm_load_print_meta: n_ctx_train      = 4096
2024-12-19 18:18:10 llm_load_print_meta: n_embd           = 4096
2024-12-19 18:18:10 llm_load_print_meta: n_layer          = 32
2024-12-19 18:18:10 llm_load_print_meta: n_head           = 32
2024-12-19 18:18:10 llm_load_print_meta: n_head_kv        = 32
2024-12-19 18:18:10 llm_load_print_meta: n_rot            = 128
2024-12-19 18:18:10 llm_load_print_meta: n_swa            = 0
2024-12-19 18:18:10 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 18:18:10 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 18:18:10 llm_load_print_meta: n_gqa            = 1
2024-12-19 18:18:10 llm_load_print_meta: n_embd_k_gqa     = 4096
2024-12-19 18:18:10 llm_load_print_meta: n_embd_v_gqa     = 4096
2024-12-19 18:18:10 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 18:18:10 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 18:18:10 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 18:18:10 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 18:18:10 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 18:18:10 llm_load_print_meta: n_ff             = 11008
2024-12-19 18:18:10 llm_load_print_meta: n_expert         = 0
2024-12-19 18:18:10 llm_load_print_meta: n_expert_used    = 0
2024-12-19 18:18:10 llm_load_print_meta: causal attn      = 1
2024-12-19 18:18:10 llm_load_print_meta: pooling type     = 0
2024-12-19 18:18:10 llm_load_print_meta: rope type        = 0
2024-12-19 18:18:10 llm_load_print_meta: rope scaling     = linear
2024-12-19 18:18:10 llm_load_print_meta: freq_base_train  = 10000.0
2024-12-19 18:18:10 llm_load_print_meta: freq_scale_train = 1
2024-12-19 18:18:10 llm_load_print_meta: n_ctx_orig_yarn  = 4096
2024-12-19 18:18:10 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 18:18:10 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 18:18:10 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 18:18:10 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 18:18:10 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 18:18:10 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 18:18:10 llm_load_print_meta: model type       = 7B
2024-12-19 18:18:10 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 18:18:10 llm_load_print_meta: model params     = 6.74 B
2024-12-19 18:18:10 llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
2024-12-19 18:18:10 llm_load_print_meta: general.name     = LLaMA v2
2024-12-19 18:18:10 llm_load_print_meta: BOS token        = 1 '<s>'
2024-12-19 18:18:10 llm_load_print_meta: EOS token        = 2 '</s>'
2024-12-19 18:18:10 llm_load_print_meta: UNK token        = 0 '<unk>'
2024-12-19 18:18:10 llm_load_print_meta: LF token         = 13 '<0x0A>'
2024-12-19 18:18:10 llm_load_print_meta: EOG token        = 2 '</s>'
2024-12-19 18:18:10 llm_load_print_meta: max token length = 48
2024-12-19 18:20:17 time=2024-12-20T02:20:17.866Z level=WARN source=server.go:562 msg="client connection closed before server finished loading, aborting load"
2024-12-19 18:20:17 time=2024-12-20T02:20:17.866Z level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
2024-12-19 18:20:23 time=2024-12-20T02:20:23.052Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.185323502 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:20:23 time=2024-12-20T02:20:23.593Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.726703414 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:20:23 time=2024-12-20T02:20:23.877Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=6.010852554 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:23:14 time=2024-12-20T02:23:14.087Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.121159527 model=/root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa
2024-12-19 18:23:14 time=2024-12-20T02:23:14.353Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.387194086 model=/root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa
2024-12-19 18:23:14 time=2024-12-20T02:23:14.622Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.656536711 model=/root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa
2024-12-19 18:23:16 time=2024-12-20T02:23:16.694Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=11632902144 required="1.6 GiB"
2024-12-19 18:23:16 time=2024-12-20T02:23:16.963Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="28.8 GiB" free_swap="8.0 GiB"
2024-12-19 18:23:16 time=2024-12-20T02:23:16.964Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=1 layers.model=33 layers.offload=1 layers.split="" memory.available="[10.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="8.7 GiB" memory.required.partial="1.6 GiB" memory.required.kv="4.0 GiB" memory.required.allocations="[1.6 GiB]" memory.weights.total="7.4 GiB" memory.weights.repeating="7.3 GiB" memory.weights.nonrepeating="102.6 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="681.0 MiB"
2024-12-19 18:23:16 time=2024-12-20T02:23:16.965Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 --ctx-size 8192 --batch-size 512 --n-gpu-layers 1 --threads 4 --parallel 4 --port 35509"
2024-12-19 18:23:16 time=2024-12-20T02:23:16.966Z level=INFO source=sched.go:449 msg="loaded runners" count=1
2024-12-19 18:23:16 time=2024-12-20T02:23:16.966Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 18:23:16 time=2024-12-20T02:23:16.966Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 18:23:17 time=2024-12-20T02:23:17.034Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 18:23:17 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 18:23:17 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 18:23:17 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 18:23:17   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 18:23:17 time=2024-12-20T02:23:17.133Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=4
2024-12-19 18:23:17 time=2024-12-20T02:23:17.133Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:35509"
2024-12-19 18:23:17 time=2024-12-20T02:23:17.218Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 18:23:17 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 18:23:17 llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))
2024-12-19 18:23:17 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 18:23:17 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 17:46:39 [GIN] 2024/12/20 - 01:46:39 | 499 |  9.284906124s |       127.0.0.1 | POST     "/api/generate"
2024-12-19 17:46:53 [GIN] 2024/12/20 - 01:46:53 | 200 |      21.355µs |       127.0.0.1 | HEAD     "/"
2024-12-19 17:46:53 [GIN] 2024/12/20 - 01:46:53 | 200 |   172.75649ms |       127.0.0.1 | POST     "/api/show"
2024-12-19 17:49:43 [GIN] 2024/12/20 - 01:49:43 | 200 |         2m39s |       127.0.0.1 | POST     "/api/generate"
2024-12-19 17:49:53 [GIN] 2024/12/20 - 01:49:53 | 200 |  988.705309ms |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:51:14 [GIN] 2024/12/20 - 01:51:14 | 200 |  5.740845555s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:51:43 [GIN] 2024/12/20 - 01:51:43 | 200 |  3.636715225s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:52:07 [GIN] 2024/12/20 - 01:52:07 | 200 |  2.657396137s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:53:01 [GIN] 2024/12/20 - 01:53:01 | 200 |  4.962642628s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:54:06 [GIN] 2024/12/20 - 01:54:06 | 200 |  3.349852644s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:54:32 [GIN] 2024/12/20 - 01:54:32 | 200 |  1.224581373s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:55:14 [GIN] 2024/12/20 - 01:55:14 | 200 |  2.935079443s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:55:25 [GIN] 2024/12/20 - 01:55:25 | 200 |   90.075114ms |      172.18.0.1 | GET      "/api/tags"
2024-12-19 17:55:44 [GIN] 2024/12/20 - 01:55:44 | 200 |  1.004790699s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:57:04 [GIN] 2024/12/20 - 01:57:04 | 200 |  2.162769462s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 17:57:33 [GIN] 2024/12/20 - 01:57:33 | 499 |          2m0s |      172.18.0.1 | POST     "/api/chat"
2024-12-19 18:01:10 [GIN] 2024/12/20 - 02:01:10 | 200 |  6.854015556s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:02:20 [GIN] 2024/12/20 - 02:02:20 | 200 |  100.841938ms |      172.18.0.1 | GET      "/api/tags"
2024-12-19 18:03:24 [GIN] 2024/12/20 - 02:03:24 | 200 |  7.775833587s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:04:29 [GIN] 2024/12/20 - 02:04:29 | 499 |          2m0s |      172.18.0.1 | POST     "/api/chat"
2024-12-19 18:06:22 [GIN] 2024/12/20 - 02:06:22 | 200 |  3.780234626s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:08:47 [GIN] 2024/12/20 - 02:08:47 | 200 |  8.755928157s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:09:45 [GIN] 2024/12/20 - 02:09:45 | 200 |  5.334099283s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:10:41 [GIN] 2024/12/20 - 02:10:41 | 200 |  3.169804786s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:11:02 [GIN] 2024/12/20 - 02:11:02 | 200 |   99.235544ms |      172.18.0.5 | GET      "/api/tags"
2024-12-19 18:12:20 [GIN] 2024/12/20 - 02:12:20 | 200 |  3.228679975s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:13:10 [GIN] 2024/12/20 - 02:13:10 | 499 |          2m0s |      172.18.0.5 | POST     "/api/chat"
2024-12-19 18:15:53 [GIN] 2024/12/20 - 02:15:53 | 200 |  4.378705097s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:17:48 [GIN] 2024/12/20 - 02:17:48 | 200 |  4.301579243s |       127.0.0.1 | POST     "/api/chat"
2024-12-19 18:18:09 [GIN] 2024/12/20 - 02:18:09 | 200 |   97.181891ms |      172.18.0.5 | GET      "/api/tags"
2024-12-19 18:20:17 [GIN] 2024/12/20 - 02:20:17 | 499 |          2m0s |      172.18.0.5 | POST     "/api/chat"
2024-12-19 18:23:16 [GIN] 2024/12/20 - 02:23:16 | 200 |  112.171796ms |      172.18.0.5 | GET      "/api/tags"
2024-12-19 18:23:17 llama_model_loader: - kv   1:                               general.name str              = LLaMA v2
2024-12-19 18:23:17 llama_model_loader: - kv   2:                       llama.context_length u32              = 4096
2024-12-19 18:23:17 llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096
2024-12-19 18:23:17 llama_model_loader: - kv   4:                          llama.block_count u32              = 32
2024-12-19 18:23:17 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008
2024-12-19 18:23:17 llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128
2024-12-19 18:23:17 llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32
2024-12-19 18:23:17 llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32
2024-12-19 18:23:17 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 18:23:17 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 18:23:17 llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama
2024-12-19 18:23:17 llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = ["<unk>", "<s>", "</s>", "<0x00>", "<...
2024-12-19 18:23:17 llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...
2024-12-19 18:23:17 llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...
2024-12-19 18:23:17 llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = ["▁ t", "e r", "i n", "▁ a", "e n...
2024-12-19 18:23:17 llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1
2024-12-19 18:23:17 llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2
2024-12-19 18:23:17 llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0
2024-12-19 18:23:17 llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true
2024-12-19 18:23:17 llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false
2024-12-19 18:23:17 llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...
2024-12-19 18:23:17 llama_model_loader: - kv  22:               general.quantization_version u32              = 2
2024-12-19 18:23:17 llama_model_loader: - type  f32:   65 tensors
2024-12-19 18:23:17 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 18:23:17 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 18:23:17 llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
2024-12-19 18:23:17 llm_load_vocab: special tokens cache size = 3
2024-12-19 18:23:17 llm_load_vocab: token to piece cache size = 0.1684 MB
2024-12-19 18:23:17 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 18:23:17 llm_load_print_meta: arch             = llama
2024-12-19 18:23:17 llm_load_print_meta: vocab type       = SPM
2024-12-19 18:23:17 llm_load_print_meta: n_vocab          = 32000
2024-12-19 18:23:17 llm_load_print_meta: n_merges         = 0
2024-12-19 18:23:17 llm_load_print_meta: vocab_only       = 0
2024-12-19 18:23:17 llm_load_print_meta: n_ctx_train      = 4096
2024-12-19 18:23:17 llm_load_print_meta: n_embd           = 4096
2024-12-19 18:23:17 llm_load_print_meta: n_layer          = 32
2024-12-19 18:23:17 llm_load_print_meta: n_head           = 32
2024-12-19 18:23:17 llm_load_print_meta: n_head_kv        = 32
2024-12-19 18:23:17 llm_load_print_meta: n_rot            = 128
2024-12-19 18:23:17 llm_load_print_meta: n_swa            = 0
2024-12-19 18:23:17 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 18:23:17 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 18:23:17 llm_load_print_meta: n_gqa            = 1
2024-12-19 18:23:17 llm_load_print_meta: n_embd_k_gqa     = 4096
2024-12-19 18:23:17 llm_load_print_meta: n_embd_v_gqa     = 4096
2024-12-19 18:23:17 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 18:23:17 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 18:23:17 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 18:23:17 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 18:23:17 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 18:23:17 llm_load_print_meta: n_ff             = 11008
2024-12-19 18:23:17 llm_load_print_meta: n_expert         = 0
2024-12-19 18:23:17 llm_load_print_meta: n_expert_used    = 0
2024-12-19 18:23:17 llm_load_print_meta: causal attn      = 1
2024-12-19 18:23:17 llm_load_print_meta: pooling type     = 0
2024-12-19 18:23:17 llm_load_print_meta: rope type        = 0
2024-12-19 18:23:17 llm_load_print_meta: rope scaling     = linear
2024-12-19 18:23:17 llm_load_print_meta: freq_base_train  = 10000.0
2024-12-19 18:23:17 llm_load_print_meta: freq_scale_train = 1
2024-12-19 18:23:17 llm_load_print_meta: n_ctx_orig_yarn  = 4096
2024-12-19 18:23:17 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 18:23:17 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 18:23:17 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 18:23:17 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 18:23:17 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 18:23:17 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 18:23:17 llm_load_print_meta: model type       = 7B
2024-12-19 18:23:17 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 18:23:17 llm_load_print_meta: model params     = 6.74 B
2024-12-19 18:23:17 llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) 
2024-12-19 18:23:17 llm_load_print_meta: general.name     = LLaMA v2
2024-12-19 18:23:17 llm_load_print_meta: BOS token        = 1 '<s>'
2024-12-19 18:23:17 llm_load_print_meta: EOS token        = 2 '</s>'
2024-12-19 18:23:17 llm_load_print_meta: UNK token        = 0 '<unk>'
2024-12-19 18:23:17 llm_load_print_meta: LF token         = 13 '<0x0A>'
2024-12-19 18:23:17 llm_load_print_meta: EOG token        = 2 '</s>'
2024-12-19 18:23:17 llm_load_print_meta: max token length = 48
2024-12-19 18:25:24 time=2024-12-20T02:25:24.716Z level=WARN source=server.go:562 msg="client connection closed before server finished loading, aborting load"
2024-12-19 18:25:24 time=2024-12-20T02:25:24.716Z level=ERROR source=sched.go:455 msg="error loading llama server" error="timed out waiting for llama runner to start: context canceled"
2024-12-19 18:25:24 time=2024-12-20T02:25:24.716Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-bed8562d-fb5d-389f-72d0-fcc5fb8676d9 library=cuda total="4.0 GiB" available="3.2 GiB"
2024-12-19 18:25:24 [GIN] 2024/12/20 - 02:25:24 | 499 |          2m0s |      172.18.0.5 | POST     "/api/chat"
2024-12-19 18:25:26 time=2024-12-20T02:25:26.448Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa gpu=GPU-00f80bb6-f1db-f53e-4a4a-27dbdc2f30ce parallel=4 available=11632902144 required="6.2 GiB"
2024-12-19 18:25:27 time=2024-12-20T02:25:27.152Z level=INFO source=server.go:104 msg="system memory" total="31.3 GiB" free="28.7 GiB" free_swap="8.0 GiB"
2024-12-19 18:25:27 time=2024-12-20T02:25:27.152Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=33 layers.offload=33 layers.split="" memory.available="[10.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="6.2 GiB" memory.required.partial="6.2 GiB" memory.required.kv="1.0 GiB" memory.required.allocations="[6.2 GiB]" memory.weights.total="4.7 GiB" memory.weights.repeating="4.3 GiB" memory.weights.nonrepeating="411.0 MiB" memory.graph.full="560.0 MiB" memory.graph.partial="677.5 MiB"
2024-12-19 18:25:27 time=2024-12-20T02:25:27.154Z level=INFO source=server.go:376 msg="starting llama server" cmd="/usr/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 8 --parallel 4 --port 42485"
2024-12-19 18:25:27 time=2024-12-20T02:25:27.155Z level=INFO source=sched.go:449 msg="loaded runners" count=1
2024-12-19 18:25:27 time=2024-12-20T02:25:27.155Z level=INFO source=server.go:555 msg="waiting for llama runner to start responding"
2024-12-19 18:25:27 time=2024-12-20T02:25:27.155Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server error"
2024-12-19 18:25:27 time=2024-12-20T02:25:27.227Z level=INFO source=runner.go:945 msg="starting go runner"
2024-12-19 18:25:27 ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
2024-12-19 18:25:27 ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
2024-12-19 18:25:27 ggml_cuda_init: found 1 CUDA devices:
2024-12-19 18:25:27   Device 0: NVIDIA GeForce RTX 4070, compute capability 8.9, VMM: yes
2024-12-19 18:25:27 time=2024-12-20T02:25:27.338Z level=INFO source=runner.go:946 msg=system info="CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)" threads=8
2024-12-19 18:25:27 time=2024-12-20T02:25:27.339Z level=INFO source=.:0 msg="Server listening on 127.0.0.1:42485"
2024-12-19 18:25:27 time=2024-12-20T02:25:27.407Z level=INFO source=server.go:589 msg="waiting for server to become available" status="llm server loading model"
2024-12-19 18:25:27 llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 4070) - 11094 MiB free
2024-12-19 18:25:27 llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-6a0746a1ec1aef3e7ec53868f220ff6e389f6f8ef87a01d77c96807de94ca2aa (version GGUF V3 (latest))
2024-12-19 18:25:27 llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
2024-12-19 18:25:27 llama_model_loader: - kv   0:                       general.architecture str              = llama
2024-12-19 18:25:27 llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct
2024-12-19 18:25:27 llama_model_loader: - kv   2:                          llama.block_count u32              = 32
2024-12-19 18:25:27 llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
2024-12-19 18:25:27 llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
2024-12-19 18:25:27 llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
2024-12-19 18:25:27 llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
2024-12-19 18:25:27 llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
2024-12-19 18:25:27 llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
2024-12-19 18:25:27 llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
2024-12-19 18:25:27 llama_model_loader: - kv  10:                          general.file_type u32              = 2
2024-12-19 18:25:27 llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
2024-12-19 18:25:27 llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
2024-12-19 18:25:27 llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
2024-12-19 18:25:27 llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
2024-12-19 18:25:27 llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
2024-12-19 18:25:27 llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
2024-12-19 18:25:27 llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
2024-12-19 18:25:27 llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
2024-12-19 18:25:27 llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009
2024-12-19 18:25:27 llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
2024-12-19 18:25:27 llama_model_loader: - kv  21:               general.quantization_version u32              = 2
2024-12-19 18:25:27 llama_model_loader: - type  f32:   65 tensors
2024-12-19 18:25:27 llama_model_loader: - type q4_0:  225 tensors
2024-12-19 18:25:27 llama_model_loader: - type q6_K:    1 tensors
2024-12-19 18:25:28 llm_load_vocab: special tokens cache size = 256
2024-12-19 18:25:28 llm_load_vocab: token to piece cache size = 0.8000 MB
2024-12-19 18:25:28 llm_load_print_meta: format           = GGUF V3 (latest)
2024-12-19 18:25:28 llm_load_print_meta: arch             = llama
2024-12-19 18:25:28 llm_load_print_meta: vocab type       = BPE
2024-12-19 18:25:28 llm_load_print_meta: n_vocab          = 128256
2024-12-19 18:25:28 llm_load_print_meta: n_merges         = 280147
2024-12-19 18:25:28 llm_load_print_meta: vocab_only       = 0
2024-12-19 18:25:28 llm_load_print_meta: n_ctx_train      = 8192
2024-12-19 18:25:28 llm_load_print_meta: n_embd           = 4096
2024-12-19 18:25:28 llm_load_print_meta: n_layer          = 32
2024-12-19 18:25:28 llm_load_print_meta: n_head           = 32
2024-12-19 18:25:28 llm_load_print_meta: n_head_kv        = 8
2024-12-19 18:25:28 llm_load_print_meta: n_rot            = 128
2024-12-19 18:25:28 llm_load_print_meta: n_swa            = 0
2024-12-19 18:25:28 llm_load_print_meta: n_embd_head_k    = 128
2024-12-19 18:25:28 llm_load_print_meta: n_embd_head_v    = 128
2024-12-19 18:25:28 llm_load_print_meta: n_gqa            = 4
2024-12-19 18:25:28 llm_load_print_meta: n_embd_k_gqa     = 1024
2024-12-19 18:25:28 llm_load_print_meta: n_embd_v_gqa     = 1024
2024-12-19 18:25:28 llm_load_print_meta: f_norm_eps       = 0.0e+00
2024-12-19 18:25:28 llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
2024-12-19 18:25:28 llm_load_print_meta: f_clamp_kqv      = 0.0e+00
2024-12-19 18:25:28 llm_load_print_meta: f_max_alibi_bias = 0.0e+00
2024-12-19 18:25:28 llm_load_print_meta: f_logit_scale    = 0.0e+00
2024-12-19 18:25:28 llm_load_print_meta: n_ff             = 14336
2024-12-19 18:25:28 llm_load_print_meta: n_expert         = 0
2024-12-19 18:25:28 llm_load_print_meta: n_expert_used    = 0
2024-12-19 18:25:28 llm_load_print_meta: causal attn      = 1
2024-12-19 18:25:28 llm_load_print_meta: pooling type     = 0
2024-12-19 18:25:28 llm_load_print_meta: rope type        = 0
2024-12-19 18:25:28 llm_load_print_meta: rope scaling     = linear
2024-12-19 18:25:28 llm_load_print_meta: freq_base_train  = 500000.0
2024-12-19 18:25:28 llm_load_print_meta: freq_scale_train = 1
2024-12-19 18:25:28 llm_load_print_meta: n_ctx_orig_yarn  = 8192
2024-12-19 18:25:28 llm_load_print_meta: rope_finetuned   = unknown
2024-12-19 18:25:28 llm_load_print_meta: ssm_d_conv       = 0
2024-12-19 18:25:28 llm_load_print_meta: ssm_d_inner      = 0
2024-12-19 18:25:28 llm_load_print_meta: ssm_d_state      = 0
2024-12-19 18:25:28 llm_load_print_meta: ssm_dt_rank      = 0
2024-12-19 18:25:28 llm_load_print_meta: ssm_dt_b_c_rms   = 0
2024-12-19 18:25:28 llm_load_print_meta: model type       = 8B
2024-12-19 18:25:28 llm_load_print_meta: model ftype      = Q4_0
2024-12-19 18:25:28 llm_load_print_meta: model params     = 8.03 B
2024-12-19 18:25:28 llm_load_print_meta: model size       = 4.33 GiB (4.64 BPW) 
2024-12-19 18:25:28 llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct
2024-12-19 18:25:28 llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
2024-12-19 18:25:28 llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
2024-12-19 18:25:28 llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
2024-12-19 18:25:28 llm_load_print_meta: LF token         = 128 'Ä'
2024-12-19 18:25:28 llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
2024-12-19 18:25:28 llm_load_print_meta: max token length = 256
2024-12-19 18:25:29 time=2024-12-20T02:25:29.792Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.075317476 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:25:30 time=2024-12-20T02:25:30.122Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.405414703 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
2024-12-19 18:25:30 time=2024-12-20T02:25:30.407Z level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.6902008859999995 model=/root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246
